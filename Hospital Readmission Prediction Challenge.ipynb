{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11568925,"sourceType":"datasetVersion","datasetId":7253253}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style=\"color:#2E86C1\">üè• Hospital Readmission Prediction Challenge</span>\n\n---\n\n## <span style=\"color:#1ABC9C\">üìå Challenge Overview</span>\n<div style=\"padding: 15px; border-radius: 5px\">\nHealthcare organizations face significant challenges in preventing avoidable hospital readmissions. This competition focuses on developing machine learning models to predict <span style=\"color:#E74C3C\">**30-day readmission risk**</span> using clinical data, helping providers:\n- <span style=\"color:#28B463\">Improve patient outcomes</span> üå±\n- <span style=\"color:#2E86C1\">Reduce healthcare costs</span> üí∞\n- <span style=\"color:#8E44AD\">Optimize care coordination</span> ü§ù\n</div>\n\n---\n\n## <span style=\"color:#1ABC9C\">üéØ Competition Objective</span>\n<div style=\"padding: 15px; border-radius: 5px; border: 0.5px solid #D4E6F1\">\n\n**Develop a binary classifier to predict:**\n\n**```Readmitted_30 = 1 if readmitted within 30 days```**\n\n   **```Readmitted_30 = 0 if no readmission```**\n","metadata":{"id":"MYlZ7ldYVJo0"}},{"cell_type":"markdown","source":"## Hospital Readmission Dataset Overview\n\n### Key Features Table\n| Column / Group        | Description                                                                 | Relationship to Readmitted_30                                                                 |\n|-----------------------|-----------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|\n| ID                    | Unique stay identifier                                                      | No predictive value; used only for record linkage                                            |\n| STAY_DRG_CD           | Diagnosis-Related Group code                                               | Complex DRGs (e.g., cardiac/respiratory) correlate with higher readmission rates            |\n| stay_length (engineered) | Days between admission & discharge                                      | Longer stays ‚Üí higher risk (indicates complications/comorbidities)                         |\n| STUS_CD               | Discharge status code                                                       | AMA/transfers show distinct readmission patterns (see discharge codes below)               |\n| TYPE_ADM              | Admission type (emergency, urgent, elective)                                | Emergency/urgent admissions ‚Üí higher risk                                                   |\n| SRC_ADMS              | Source of admission (ER, referral, transfer)                                | ER/transfers from post-acute care ‚Üí distinct risk profiles                                  |\n| AD_DGNS               | Admitting diagnosis code (ICD)                                              | Initial diagnosis (e.g., sepsis, CHF) strongly influences readmission                      |\n| DGNSCD01-DGNSCD25     | 25 diagnosis codes                                                          | Comorbidity burden ‚Üí higher risk; specific codes (e.g., CKD) are risk factors               |\n| PRCDRCD01-PRCDRCD25   | 25 procedure codes                                                          | Major surgeries ‚Üí higher complication/readmission rates                                     |\n| Readmitted_30         | **Target**: 1 if readmitted within 30 days, else 0                          |                                                                                             |\n\n---\n\n### Discharge Status Codes (STUS_CD)\n| Code | Definition                                      | Readmission Risk          |\n|------|-------------------------------------------------|---------------------------|\n| 1    | Discharged to home/self-care                    | Lowest risk               |\n| 3    | Transferred to SNF (Medicare-certified)         | Moderate risk             |\n| 6    | Home with home health services                  | Moderate‚Äìhigh risk        |\n| 62   | Transferred to inpatient rehabilitation (IRF)   | High risk                 |\n| 63   | Transferred to long-term care hospital (LTCH)   | Very high risk            |\n\n---\n\n### Admission Types (TYPE_ADM)\n| Code | Definition                                      | Readmission Risk          |\n|------|-------------------------------------------------|---------------------------|\n| 1    | Emergency admission                             | High risk                 |\n| 2    | Urgent admission                                | High risk                 |\n| 3    | Elective admission                              | Lower risk                |\n| 5    | Trauma Center admission                         | Very High risk            |\n| 9    | Information Not Available                       | Unknown risk              |\n\n---\n\n### Admission Sources (SRC_ADMS)\n| Code | Definition                                      | Readmission Risk          |\n|------|-------------------------------------------------|---------------------------|\n| 1    | Physician Referral                              | Moderate risk             |\n| 2    | Clinic Referral                                 | Moderate risk             |\n| 5    | Transfer from SNF/ICF                           | High risk                 |\n\n\n---\n\n### Diagnosis and Procedures Code (DGNSCD01-25, PRCDRCD01-25)\n| Column          | Description                                      | Relationship to Readmitted_30                                                                 |\n|-----------------|--------------------------------------------------|-----------------------------------------------------------------------------------------------|\n| **DGNSCD01**    | Primary diagnosis code (ICD-10)                  | Primary condition driving the stay; often the strongest predictor. High-risk primaries (e.g., CHF, sepsis) ‚Üë readmission. |\n| **DGNSCD02**    | Secondary diagnosis code                         | Comorbidity #1: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD03**    | Secondary diagnosis code                         | Comorbidity #2: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD04**    | Secondary diagnosis code                         | Comorbidity #3: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD05**    | Secondary diagnosis code                         | Comorbidity #4: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD06**    | Secondary diagnosis code                         | Comorbidity #5: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD07**    | Secondary diagnosis code                         | Comorbidity #6: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD08**    | Secondary diagnosis code                         | Comorbidity #7: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD09**    | Secondary diagnosis code                         | Comorbidity #8: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD10**    | Secondary diagnosis code                         | Comorbidity #9: adds to overall burden; presence ‚Üë readmission risk.                          |\n| **DGNSCD11**    | Secondary diagnosis code                         | Comorbidity #10: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD12**    | Secondary diagnosis code                         | Comorbidity #11: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD13**    | Secondary diagnosis code                         | Comorbidity #12: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD14**    | Secondary diagnosis code                         | Comorbidity #13: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD15**    | Secondary diagnosis code                         | Comorbidity #14: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD16**    | Secondary diagnosis code                         | Comorbidity #15: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD17**    | Secondary diagnosis code                         | Comorbidity #16: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD18**    | Secondary diagnosis code                         | Comorbidity #17: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD19**    | Secondary diagnosis code                         | Comorbidity #18: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD20**    | Secondary diagnosis code                         | Comorbidity #19: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD21**    | Secondary diagnosis code                         | Comorbidity #20: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD22**    | Secondary diagnosis code                         | Comorbidity #21: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD23**    | Secondary diagnosis code                         | Comorbidity #22: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD24**    | Secondary diagnosis code                         | Comorbidity #23: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **DGNSCD25**    | Secondary diagnosis code                         | Comorbidity #24: adds to overall burden; presence ‚Üë readmission risk.                         |\n| **PRCDRCD01**   | Primary procedure code (ICD-10-PCS)              | Primary intervention; major surgeries or invasive procedures ‚Üë readmission likelihood.        |\n| **PRCDRCD02**   | Secondary procedure code                          | Procedure #2: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD03**   | Secondary procedure code                          | Procedure #3: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD04**   | Secondary procedure code                          | Procedure #4: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD05**   | Secondary procedure code                          | Procedure #5: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD06**   | Secondary procedure code                          | Procedure #6: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD07**   | Secondary procedure code                          | Procedure #7: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD08**   | Secondary procedure code                          | Procedure #8: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD09**   | Secondary procedure code                          | Procedure #9: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD10**   | Secondary procedure code                          | Procedure #10: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD11**   | Secondary procedure code                          | Procedure #11: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD12**   | Secondary procedure code                          | Procedure #12: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD13**   | Secondary procedure code                          | Procedure #13: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD14**   | Secondary procedure code                          | Procedure #14: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD15**   | Secondary procedure code                          | Procedure #15: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD16**   | Secondary procedure code                          | Procedure #16: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD17**   | Secondary procedure code                          | Procedure #17: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD18**   | Secondary procedure code                          | Procedure #18: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD19**   | Secondary procedure code                          | Procedure #19: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD20**   | Secondary procedure code                          | Procedure #20: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD21**   | Secondary procedure code                          | Procedure #21: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD22**   | Secondary procedure code                          | Procedure #22: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD23**   | Secondary procedure code                          | Procedure #23: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD24**   | Secondary procedure code                          | Procedure #24: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |\n| **PRCDRCD25**   | Secondary procedure code                          | Procedure #25: additional interventions; more procedures signal complexity and ‚Üë readmission risk. |","metadata":{"id":"bp0usfdYVJo1"}},{"cell_type":"markdown","source":"# ‚úÖ Data Preparation","metadata":{"id":"7hbwbGRAVJo3"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import (f1_score, roc_auc_score, precision_score, recall_score, roc_curve, auc)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport optuna\nfrom catboost import CatBoostClassifier\n\n# Settings\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.seterr(all='ignore')","metadata":{"trusted":true,"id":"4Pr_xxfkVJo3","outputId":"425a1028-4b50-4166-a2a1-cee91957c177","execution":{"iopub.status.busy":"2025-04-25T21:24:08.878160Z","iopub.execute_input":"2025-04-25T21:24:08.878481Z","iopub.status.idle":"2025-04-25T21:24:10.844044Z","shell.execute_reply.started":"2025-04-25T21:24:08.878455Z","shell.execute_reply":"2025-04-25T21:24:10.843242Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport torch\nprint(\"CUDA Available:\", torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T21:24:12.605995Z","iopub.execute_input":"2025-04-25T21:24:12.606479Z","iopub.status.idle":"2025-04-25T21:24:17.329733Z","shell.execute_reply.started":"2025-04-25T21:24:12.606452Z","shell.execute_reply":"2025-04-25T21:24:17.328890Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/softec-25-machine-learning-competition/sample_submission.csv\n/kaggle/input/softec-25-machine-learning-competition/train.csv\n/kaggle/input/softec-25-machine-learning-competition/metaData.csv\n/kaggle/input/softec-25-machine-learning-competition/test.csv\nCUDA Available: False\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define paths\ntrain_csv_path = '/kaggle/input/softec-25-machine-learning-competition/train.csv'\ntest_csv_path = '/kaggle/input/softec-25-machine-learning-competition/test.csv'\n\ndf_train = pd.read_csv(train_csv_path)\ndf_test = pd.read_csv(test_csv_path)","metadata":{"trusted":true,"id":"ec0JfM74VJo5","execution":{"iopub.status.busy":"2025-04-25T21:24:20.597952Z","iopub.execute_input":"2025-04-25T21:24:20.598756Z","iopub.status.idle":"2025-04-25T21:24:22.105032Z","shell.execute_reply.started":"2025-04-25T21:24:20.598727Z","shell.execute_reply":"2025-04-25T21:24:22.104149Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# peek the data\ndf_train.head()","metadata":{"trusted":true,"id":"dhz-CWUlVJo6","outputId":"81df6251-76e4-4340-ae0d-908614177c13","execution":{"iopub.status.busy":"2025-04-25T21:24:23.091552Z","iopub.execute_input":"2025-04-25T21:24:23.091866Z","iopub.status.idle":"2025-04-25T21:24:23.109649Z","shell.execute_reply.started":"2025-04-25T21:24:23.091840Z","shell.execute_reply":"2025-04-25T21:24:23.108889Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      ID  STAY_DRG_CD         STAY_FROM_DT         STAY_THRU_DT  STUS_CD  \\\n0  17319          NaN  2017-12-13 00:00:00  2017-12-20 00:00:00       62   \n1  19722          NaN  2017-10-19 00:00:00  2017-10-23 00:00:00        1   \n2  89699          NaN  2018-08-06 00:00:00  2018-08-08 00:00:00        1   \n3   8086          NaN  2016-12-20 00:00:00  2016-12-27 00:00:00       62   \n4  68049          NaN  2016-01-06 00:00:00  2016-01-12 00:00:00        6   \n\n   TYPE_ADM  SRC_ADMS AD_DGNS DGNSCD01 PRCDRCD01  ... DGNSCD22 PRCDRCD22  \\\n0         1         2  M25551  S72001A   0SRR01Z  ...     Z803       NaN   \n1         1         1    R531     A419       NaN  ...      NaN       NaN   \n2         1         1    R002     J690       NaN  ...      NaN       NaN   \n3         5         1    K661     K661       NaN  ...     I440       NaN   \n4         1         1   J9601     J690       NaN  ...      NaN       NaN   \n\n  DGNSCD23 PRCDRCD23 DGNSCD24 PRCDRCD24 DGNSCD25 PRCDRCD25 stay_drg_cd  \\\n0   Z86711       NaN   Z86718       NaN   Z85828       NaN         469   \n1      NaN       NaN      NaN       NaN      NaN       NaN         871   \n2      NaN       NaN      NaN       NaN      NaN       NaN         177   \n3      NaN       NaN      NaN       NaN      NaN       NaN         393   \n4      NaN       NaN      NaN       NaN      NaN       NaN         177   \n\n  Readmitted_30  \n0             0  \n1             1  \n2             0  \n3             0  \n4             0  \n\n[5 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>STAY_DRG_CD</th>\n      <th>STAY_FROM_DT</th>\n      <th>STAY_THRU_DT</th>\n      <th>STUS_CD</th>\n      <th>TYPE_ADM</th>\n      <th>SRC_ADMS</th>\n      <th>AD_DGNS</th>\n      <th>DGNSCD01</th>\n      <th>PRCDRCD01</th>\n      <th>...</th>\n      <th>DGNSCD22</th>\n      <th>PRCDRCD22</th>\n      <th>DGNSCD23</th>\n      <th>PRCDRCD23</th>\n      <th>DGNSCD24</th>\n      <th>PRCDRCD24</th>\n      <th>DGNSCD25</th>\n      <th>PRCDRCD25</th>\n      <th>stay_drg_cd</th>\n      <th>Readmitted_30</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17319</td>\n      <td>NaN</td>\n      <td>2017-12-13 00:00:00</td>\n      <td>2017-12-20 00:00:00</td>\n      <td>62</td>\n      <td>1</td>\n      <td>2</td>\n      <td>M25551</td>\n      <td>S72001A</td>\n      <td>0SRR01Z</td>\n      <td>...</td>\n      <td>Z803</td>\n      <td>NaN</td>\n      <td>Z86711</td>\n      <td>NaN</td>\n      <td>Z86718</td>\n      <td>NaN</td>\n      <td>Z85828</td>\n      <td>NaN</td>\n      <td>469</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19722</td>\n      <td>NaN</td>\n      <td>2017-10-19 00:00:00</td>\n      <td>2017-10-23 00:00:00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>R531</td>\n      <td>A419</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>871</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>89699</td>\n      <td>NaN</td>\n      <td>2018-08-06 00:00:00</td>\n      <td>2018-08-08 00:00:00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>R002</td>\n      <td>J690</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>177</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8086</td>\n      <td>NaN</td>\n      <td>2016-12-20 00:00:00</td>\n      <td>2016-12-27 00:00:00</td>\n      <td>62</td>\n      <td>5</td>\n      <td>1</td>\n      <td>K661</td>\n      <td>K661</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>I440</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>393</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68049</td>\n      <td>NaN</td>\n      <td>2016-01-06 00:00:00</td>\n      <td>2016-01-12 00:00:00</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>J9601</td>\n      <td>J690</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>177</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 60 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df_train.describe(include='all')","metadata":{"trusted":true,"id":"YjAyc4QOVJo6","outputId":"c0329234-33af-49e6-d886-342794fbaf92","execution":{"iopub.status.busy":"2025-04-25T21:24:26.385747Z","iopub.execute_input":"2025-04-25T21:24:26.386410Z","iopub.status.idle":"2025-04-25T21:24:27.135050Z","shell.execute_reply.started":"2025-04-25T21:24:26.386380Z","shell.execute_reply":"2025-04-25T21:24:27.134160Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                   ID  STAY_DRG_CD         STAY_FROM_DT         STAY_THRU_DT  \\\ncount   130296.000000  3798.000000               130296               130296   \nunique            NaN          NaN                 3213                 3214   \ntop               NaN          NaN  2017-01-09 00:00:00  2018-01-11 00:00:00   \nfreq              NaN          NaN                  132                  141   \nmean     81315.674833   430.033175                  NaN                  NaN   \nstd      47062.963000   230.927115                  NaN                  NaN   \nmin          2.000000    23.000000                  NaN                  NaN   \n25%      40507.750000   291.000000                  NaN                  NaN   \n50%      81315.500000   309.000000                  NaN                  NaN   \n75%     122083.500000   682.000000                  NaN                  NaN   \nmax     162871.000000   981.000000                  NaN                  NaN   \n\n              STUS_CD       TYPE_ADM       SRC_ADMS AD_DGNS DGNSCD01  \\\ncount   130296.000000  130296.000000  130296.000000  130296   130296   \nunique            NaN            NaN            NaN    2260     2691   \ntop               NaN            NaN            NaN   R0602     A419   \nfreq              NaN            NaN            NaN   24865    28655   \nmean         7.427434       1.263838       1.482478     NaN      NaN   \nstd         16.370187       0.649099       1.200835     NaN      NaN   \nmin          1.000000       1.000000       1.000000     NaN      NaN   \n25%          1.000000       1.000000       1.000000     NaN      NaN   \n50%          3.000000       1.000000       1.000000     NaN      NaN   \n75%          6.000000       1.000000       1.000000     NaN      NaN   \nmax         63.000000       9.000000       5.000000     NaN      NaN   \n\n       PRCDRCD01  ... DGNSCD22 PRCDRCD22 DGNSCD23 PRCDRCD23 DGNSCD24  \\\ncount      61628  ...    41252      4423    36587      4409    32382   \nunique      2596  ...     1826        28     1698        23     1603   \ntop      5A09357  ...        -         -        -         -        -   \nfreq        4650  ...     4365      4365     4365      4365     4365   \nmean         NaN  ...      NaN       NaN      NaN       NaN      NaN   \nstd          NaN  ...      NaN       NaN      NaN       NaN      NaN   \nmin          NaN  ...      NaN       NaN      NaN       NaN      NaN   \n25%          NaN  ...      NaN       NaN      NaN       NaN      NaN   \n50%          NaN  ...      NaN       NaN      NaN       NaN      NaN   \n75%          NaN  ...      NaN       NaN      NaN       NaN      NaN   \nmax          NaN  ...      NaN       NaN      NaN       NaN      NaN   \n\n       PRCDRCD24 DGNSCD25 PRCDRCD25 stay_drg_cd  Readmitted_30  \ncount       4399    20261      4388      126498  130296.000000  \nunique        18     1286        15         520            NaN  \ntop            -        -         -         871            NaN  \nfreq        4365     4365      4365       27887            NaN  \nmean         NaN      NaN       NaN         NaN       0.204903  \nstd          NaN      NaN       NaN         NaN       0.403632  \nmin          NaN      NaN       NaN         NaN       0.000000  \n25%          NaN      NaN       NaN         NaN       0.000000  \n50%          NaN      NaN       NaN         NaN       0.000000  \n75%          NaN      NaN       NaN         NaN       0.000000  \nmax          NaN      NaN       NaN         NaN       1.000000  \n\n[11 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>STAY_DRG_CD</th>\n      <th>STAY_FROM_DT</th>\n      <th>STAY_THRU_DT</th>\n      <th>STUS_CD</th>\n      <th>TYPE_ADM</th>\n      <th>SRC_ADMS</th>\n      <th>AD_DGNS</th>\n      <th>DGNSCD01</th>\n      <th>PRCDRCD01</th>\n      <th>...</th>\n      <th>DGNSCD22</th>\n      <th>PRCDRCD22</th>\n      <th>DGNSCD23</th>\n      <th>PRCDRCD23</th>\n      <th>DGNSCD24</th>\n      <th>PRCDRCD24</th>\n      <th>DGNSCD25</th>\n      <th>PRCDRCD25</th>\n      <th>stay_drg_cd</th>\n      <th>Readmitted_30</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>130296.000000</td>\n      <td>3798.000000</td>\n      <td>130296</td>\n      <td>130296</td>\n      <td>130296.000000</td>\n      <td>130296.000000</td>\n      <td>130296.000000</td>\n      <td>130296</td>\n      <td>130296</td>\n      <td>61628</td>\n      <td>...</td>\n      <td>41252</td>\n      <td>4423</td>\n      <td>36587</td>\n      <td>4409</td>\n      <td>32382</td>\n      <td>4399</td>\n      <td>20261</td>\n      <td>4388</td>\n      <td>126498</td>\n      <td>130296.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3213</td>\n      <td>3214</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2260</td>\n      <td>2691</td>\n      <td>2596</td>\n      <td>...</td>\n      <td>1826</td>\n      <td>28</td>\n      <td>1698</td>\n      <td>23</td>\n      <td>1603</td>\n      <td>18</td>\n      <td>1286</td>\n      <td>15</td>\n      <td>520</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2017-01-09 00:00:00</td>\n      <td>2018-01-11 00:00:00</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>R0602</td>\n      <td>A419</td>\n      <td>5A09357</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>871</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>132</td>\n      <td>141</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>24865</td>\n      <td>28655</td>\n      <td>4650</td>\n      <td>...</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>4365</td>\n      <td>27887</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>81315.674833</td>\n      <td>430.033175</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.427434</td>\n      <td>1.263838</td>\n      <td>1.482478</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.204903</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47062.963000</td>\n      <td>230.927115</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16.370187</td>\n      <td>0.649099</td>\n      <td>1.200835</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.403632</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.000000</td>\n      <td>23.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>40507.750000</td>\n      <td>291.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>81315.500000</td>\n      <td>309.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>122083.500000</td>\n      <td>682.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>162871.000000</td>\n      <td>981.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>63.000000</td>\n      <td>9.000000</td>\n      <td>5.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows √ó 60 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(\"\\nRaw Training Data Shape:\", df_train.shape)\nprint(\"Raw Test Data Shape:\", df_test.shape)","metadata":{"trusted":true,"id":"M_DwY2M2VJo6","outputId":"434c2cd5-5548-43bf-8ba0-6a69094f491b","execution":{"iopub.status.busy":"2025-04-25T21:24:32.231372Z","iopub.execute_input":"2025-04-25T21:24:32.231974Z","iopub.status.idle":"2025-04-25T21:24:32.236621Z","shell.execute_reply.started":"2025-04-25T21:24:32.231944Z","shell.execute_reply":"2025-04-25T21:24:32.235677Z"}},"outputs":[{"name":"stdout","text":"\nRaw Training Data Shape: (130296, 60)\nRaw Test Data Shape: (32575, 59)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"Duplicates:\", df_train.duplicated().sum())\nprint(\"Duplicates:\", df_test.duplicated().sum())","metadata":{"trusted":true,"id":"NjyEp0BAVJo7","outputId":"2fe90b90-f5d6-46e8-d9b5-2fae1eced661","execution":{"iopub.status.busy":"2025-04-25T21:24:35.458119Z","iopub.execute_input":"2025-04-25T21:24:35.458433Z","iopub.status.idle":"2025-04-25T21:24:36.072497Z","shell.execute_reply.started":"2025-04-25T21:24:35.458411Z","shell.execute_reply":"2025-04-25T21:24:36.071684Z"}},"outputs":[{"name":"stdout","text":"Duplicates: 0\nDuplicates: 0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(\"Raw Training Data Info:\")\ndf_train.info()\nprint(\"\\nRaw Test Data Info:\")\ndf_test.info()","metadata":{"trusted":true,"id":"VIQ_tchFVJo7","outputId":"75bad02d-b914-4c12-bd79-9c4b8dd56420","execution":{"iopub.status.busy":"2025-04-25T21:24:38.054288Z","iopub.execute_input":"2025-04-25T21:24:38.054590Z","iopub.status.idle":"2025-04-25T21:24:38.424498Z","shell.execute_reply.started":"2025-04-25T21:24:38.054569Z","shell.execute_reply":"2025-04-25T21:24:38.423552Z"}},"outputs":[{"name":"stdout","text":"Raw Training Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 130296 entries, 0 to 130295\nData columns (total 60 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   ID             130296 non-null  int64  \n 1   STAY_DRG_CD    3798 non-null    float64\n 2   STAY_FROM_DT   130296 non-null  object \n 3   STAY_THRU_DT   130296 non-null  object \n 4   STUS_CD        130296 non-null  int64  \n 5   TYPE_ADM       130296 non-null  int64  \n 6   SRC_ADMS       130296 non-null  int64  \n 7   AD_DGNS        130296 non-null  object \n 8   DGNSCD01       130296 non-null  object \n 9   PRCDRCD01      61628 non-null   object \n 10  DGNSCD02       130153 non-null  object \n 11  PRCDRCD02      38509 non-null   object \n 12  DGNSCD03       129937 non-null  object \n 13  PRCDRCD03      25879 non-null   object \n 14  DGNSCD04       129532 non-null  object \n 15  PRCDRCD04      17840 non-null   object \n 16  DGNSCD05       128889 non-null  object \n 17  PRCDRCD05      12711 non-null   object \n 18  DGNSCD06       127827 non-null  object \n 19  PRCDRCD06      9781 non-null    object \n 20  DGNSCD07       126210 non-null  object \n 21  PRCDRCD07      7633 non-null    object \n 22  DGNSCD08       124011 non-null  object \n 23  PRCDRCD08      6653 non-null    object \n 24  DGNSCD09       121084 non-null  object \n 25  PRCDRCD09      6045 non-null    object \n 26  DGNSCD10       117367 non-null  object \n 27  PRCDRCD10      5643 non-null    object \n 28  DGNSCD11       112946 non-null  object \n 29  PRCDRCD11      5298 non-null    object \n 30  DGNSCD12       107824 non-null  object \n 31  PRCDRCD12      5064 non-null    object \n 32  DGNSCD13       102240 non-null  object \n 33  PRCDRCD13      4891 non-null    object \n 34  DGNSCD14       96083 non-null   object \n 35  PRCDRCD14      4762 non-null    object \n 36  DGNSCD15       89790 non-null   object \n 37  PRCDRCD15      4648 non-null    object \n 38  DGNSCD16       83040 non-null   object \n 39  PRCDRCD16      4582 non-null    object \n 40  DGNSCD17       76270 non-null   object \n 41  PRCDRCD17      4536 non-null    object \n 42  DGNSCD18       69316 non-null   object \n 43  PRCDRCD18      4506 non-null    object \n 44  DGNSCD19       57051 non-null   object \n 45  PRCDRCD19      4475 non-null    object \n 46  DGNSCD20       51453 non-null   object \n 47  PRCDRCD20      4454 non-null    object \n 48  DGNSCD21       46322 non-null   object \n 49  PRCDRCD21      4437 non-null    object \n 50  DGNSCD22       41252 non-null   object \n 51  PRCDRCD22      4423 non-null    object \n 52  DGNSCD23       36587 non-null   object \n 53  PRCDRCD23      4409 non-null    object \n 54  DGNSCD24       32382 non-null   object \n 55  PRCDRCD24      4399 non-null    object \n 56  DGNSCD25       20261 non-null   object \n 57  PRCDRCD25      4388 non-null    object \n 58  stay_drg_cd    126498 non-null  object \n 59  Readmitted_30  130296 non-null  int64  \ndtypes: float64(1), int64(5), object(54)\nmemory usage: 59.6+ MB\n\nRaw Test Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 32575 entries, 0 to 32574\nData columns (total 59 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   ID            32575 non-null  int64  \n 1   STAY_DRG_CD   913 non-null    float64\n 2   STAY_FROM_DT  32575 non-null  object \n 3   STAY_THRU_DT  32575 non-null  object \n 4   STUS_CD       32575 non-null  int64  \n 5   TYPE_ADM      32575 non-null  int64  \n 6   SRC_ADMS      32575 non-null  int64  \n 7   AD_DGNS       32575 non-null  object \n 8   DGNSCD01      32575 non-null  object \n 9   PRCDRCD01     15305 non-null  object \n 10  DGNSCD02      32545 non-null  object \n 11  PRCDRCD02     9492 non-null   object \n 12  DGNSCD03      32492 non-null  object \n 13  PRCDRCD03     6431 non-null   object \n 14  DGNSCD04      32393 non-null  object \n 15  PRCDRCD04     4475 non-null   object \n 16  DGNSCD05      32239 non-null  object \n 17  PRCDRCD05     3228 non-null   object \n 18  DGNSCD06      31963 non-null  object \n 19  PRCDRCD06     2473 non-null   object \n 20  DGNSCD07      31591 non-null  object \n 21  PRCDRCD07     1928 non-null   object \n 22  DGNSCD08      31065 non-null  object \n 23  PRCDRCD08     1701 non-null   object \n 24  DGNSCD09      30350 non-null  object \n 25  PRCDRCD09     1539 non-null   object \n 26  DGNSCD10      29446 non-null  object \n 27  PRCDRCD10     1443 non-null   object \n 28  DGNSCD11      28385 non-null  object \n 29  PRCDRCD11     1353 non-null   object \n 30  DGNSCD12      27164 non-null  object \n 31  PRCDRCD12     1281 non-null   object \n 32  DGNSCD13      25731 non-null  object \n 33  PRCDRCD13     1244 non-null   object \n 34  DGNSCD14      24062 non-null  object \n 35  PRCDRCD14     1220 non-null   object \n 36  DGNSCD15      22444 non-null  object \n 37  PRCDRCD15     1199 non-null   object \n 38  DGNSCD16      20763 non-null  object \n 39  PRCDRCD16     1188 non-null   object \n 40  DGNSCD17      19121 non-null  object \n 41  PRCDRCD17     1182 non-null   object \n 42  DGNSCD18      17402 non-null  object \n 43  PRCDRCD18     1170 non-null   object \n 44  DGNSCD19      14230 non-null  object \n 45  PRCDRCD19     1168 non-null   object \n 46  DGNSCD20      12817 non-null  object \n 47  PRCDRCD20     1162 non-null   object \n 48  DGNSCD21      11521 non-null  object \n 49  PRCDRCD21     1160 non-null   object \n 50  DGNSCD22      10300 non-null  object \n 51  PRCDRCD22     1158 non-null   object \n 52  DGNSCD23      9159 non-null   object \n 53  PRCDRCD23     1154 non-null   object \n 54  DGNSCD24      8109 non-null   object \n 55  PRCDRCD24     1154 non-null   object \n 56  DGNSCD25      5117 non-null   object \n 57  PRCDRCD25     1153 non-null   object \n 58  stay_drg_cd   31662 non-null  object \ndtypes: float64(1), int64(4), object(54)\nmemory usage: 14.7+ MB\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\nTraining Missing Values(%):\")\nprint((df_train.isnull().sum()/len(df_train) * 100).sort_values(ascending=False))","metadata":{"trusted":true,"id":"FoW4CzZCVJo7","outputId":"a824e963-a0a2-4b36-97a1-e5d7d4e02e42","execution":{"iopub.status.busy":"2025-04-25T21:24:43.205665Z","iopub.execute_input":"2025-04-25T21:24:43.205982Z","iopub.status.idle":"2025-04-25T21:24:43.498125Z","shell.execute_reply.started":"2025-04-25T21:24:43.205959Z","shell.execute_reply":"2025-04-25T21:24:43.497157Z"}},"outputs":[{"name":"stdout","text":"\nTraining Missing Values(%):\nSTAY_DRG_CD      97.085099\nPRCDRCD25        96.632283\nPRCDRCD24        96.623841\nPRCDRCD23        96.616166\nPRCDRCD22        96.605422\nPRCDRCD21        96.594677\nPRCDRCD20        96.581630\nPRCDRCD19        96.565512\nPRCDRCD18        96.541720\nPRCDRCD17        96.518696\nPRCDRCD16        96.483392\nPRCDRCD15        96.432738\nPRCDRCD14        96.345245\nPRCDRCD13        96.246239\nPRCDRCD12        96.113465\nPRCDRCD11        95.933874\nPRCDRCD10        95.669092\nPRCDRCD09        95.360564\nPRCDRCD08        94.893934\nPRCDRCD07        94.141800\nPRCDRCD06        92.493246\nPRCDRCD05        90.244520\nPRCDRCD04        86.308098\nDGNSCD25         84.450021\nPRCDRCD03        80.138300\nDGNSCD24         75.147357\nDGNSCD23         71.920090\nPRCDRCD02        70.444987\nDGNSCD22         68.339780\nDGNSCD21         64.448640\nDGNSCD20         60.510683\nDGNSCD19         56.214312\nPRCDRCD01        52.701541\nDGNSCD18         46.801130\nDGNSCD17         41.464051\nDGNSCD16         36.268189\nDGNSCD15         31.087677\nDGNSCD14         26.257905\nDGNSCD13         21.532511\nDGNSCD12         17.246884\nDGNSCD11         13.315835\nDGNSCD10          9.922791\nDGNSCD09          7.070056\nDGNSCD08          4.823632\nDGNSCD07          3.135937\nstay_drg_cd       2.914901\nDGNSCD06          1.894916\nDGNSCD05          1.079849\nDGNSCD04          0.586357\nDGNSCD03          0.275526\nDGNSCD02          0.109750\nID                0.000000\nDGNSCD01          0.000000\nAD_DGNS           0.000000\nSRC_ADMS          0.000000\nTYPE_ADM          0.000000\nSTUS_CD           0.000000\nSTAY_THRU_DT      0.000000\nSTAY_FROM_DT      0.000000\nReadmitted_30     0.000000\ndtype: float64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"üîç Missing columns in Training Data:\")\nprint(df_train.isnull().sum()[df_train.isnull().sum() > 0])\nprint(\"\\nüîç Missing columns in Testing Data:\")\nprint(df_test.isnull().sum()[df_test.isnull().sum() > 0])","metadata":{"trusted":true,"id":"R9DR67RoVJo8","outputId":"0f8e887c-fbc9-4eeb-beec-1789042e9ab0","execution":{"iopub.status.busy":"2025-04-25T21:24:46.120656Z","iopub.execute_input":"2025-04-25T21:24:46.121532Z","iopub.status.idle":"2025-04-25T21:24:46.818800Z","shell.execute_reply.started":"2025-04-25T21:24:46.121504Z","shell.execute_reply":"2025-04-25T21:24:46.817870Z"}},"outputs":[{"name":"stdout","text":"üîç Missing columns in Training Data:\nSTAY_DRG_CD    126498\nPRCDRCD01       68668\nDGNSCD02          143\nPRCDRCD02       91787\nDGNSCD03          359\nPRCDRCD03      104417\nDGNSCD04          764\nPRCDRCD04      112456\nDGNSCD05         1407\nPRCDRCD05      117585\nDGNSCD06         2469\nPRCDRCD06      120515\nDGNSCD07         4086\nPRCDRCD07      122663\nDGNSCD08         6285\nPRCDRCD08      123643\nDGNSCD09         9212\nPRCDRCD09      124251\nDGNSCD10        12929\nPRCDRCD10      124653\nDGNSCD11        17350\nPRCDRCD11      124998\nDGNSCD12        22472\nPRCDRCD12      125232\nDGNSCD13        28056\nPRCDRCD13      125405\nDGNSCD14        34213\nPRCDRCD14      125534\nDGNSCD15        40506\nPRCDRCD15      125648\nDGNSCD16        47256\nPRCDRCD16      125714\nDGNSCD17        54026\nPRCDRCD17      125760\nDGNSCD18        60980\nPRCDRCD18      125790\nDGNSCD19        73245\nPRCDRCD19      125821\nDGNSCD20        78843\nPRCDRCD20      125842\nDGNSCD21        83974\nPRCDRCD21      125859\nDGNSCD22        89044\nPRCDRCD22      125873\nDGNSCD23        93709\nPRCDRCD23      125887\nDGNSCD24        97914\nPRCDRCD24      125897\nDGNSCD25       110035\nPRCDRCD25      125908\nstay_drg_cd      3798\ndtype: int64\n\nüîç Missing columns in Testing Data:\nSTAY_DRG_CD    31662\nPRCDRCD01      17270\nDGNSCD02          30\nPRCDRCD02      23083\nDGNSCD03          83\nPRCDRCD03      26144\nDGNSCD04         182\nPRCDRCD04      28100\nDGNSCD05         336\nPRCDRCD05      29347\nDGNSCD06         612\nPRCDRCD06      30102\nDGNSCD07         984\nPRCDRCD07      30647\nDGNSCD08        1510\nPRCDRCD08      30874\nDGNSCD09        2225\nPRCDRCD09      31036\nDGNSCD10        3129\nPRCDRCD10      31132\nDGNSCD11        4190\nPRCDRCD11      31222\nDGNSCD12        5411\nPRCDRCD12      31294\nDGNSCD13        6844\nPRCDRCD13      31331\nDGNSCD14        8513\nPRCDRCD14      31355\nDGNSCD15       10131\nPRCDRCD15      31376\nDGNSCD16       11812\nPRCDRCD16      31387\nDGNSCD17       13454\nPRCDRCD17      31393\nDGNSCD18       15173\nPRCDRCD18      31405\nDGNSCD19       18345\nPRCDRCD19      31407\nDGNSCD20       19758\nPRCDRCD20      31413\nDGNSCD21       21054\nPRCDRCD21      31415\nDGNSCD22       22275\nPRCDRCD22      31417\nDGNSCD23       23416\nPRCDRCD23      31421\nDGNSCD24       24466\nPRCDRCD24      31421\nDGNSCD25       27458\nPRCDRCD25      31422\nstay_drg_cd      913\ndtype: int64\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def preprocess_df(df):\n    df = df.copy()\n    df.drop(columns=['ID', 'STAY_DRG_CD', 'STAY_FROM_DT', 'STAY_THRU_DT'], inplace=True, errors='ignore')\n    df.dropna(subset=['stay_drg_cd'], inplace=True)\n    df['stay_drg_cd'] = pd.to_numeric(df['stay_drg_cd'], errors='coerce').astype('Int64')\n    df.dropna(subset=['stay_drg_cd'], inplace=True)\n    return df","metadata":{"trusted":true,"id":"ZCH3i3ttVJo8","execution":{"iopub.status.busy":"2025-04-25T21:24:51.323884Z","iopub.execute_input":"2025-04-25T21:24:51.324269Z","iopub.status.idle":"2025-04-25T21:24:51.329754Z","shell.execute_reply.started":"2025-04-25T21:24:51.324244Z","shell.execute_reply":"2025-04-25T21:24:51.328926Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_train = preprocess_df(df_train)","metadata":{"trusted":true,"id":"u1IoQDUtVJo8","execution":{"iopub.status.busy":"2025-04-25T21:24:53.509712Z","iopub.execute_input":"2025-04-25T21:24:53.510021Z","iopub.status.idle":"2025-04-25T21:24:53.914464Z","shell.execute_reply.started":"2025-04-25T21:24:53.509998Z","shell.execute_reply":"2025-04-25T21:24:53.913631Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"categorical_cols = ['STUS_CD', 'TYPE_ADM', 'SRC_ADMS', 'AD_DGNS', 'stay_drg_cd']\n\nfor col in categorical_cols:\n    value_counts = df_train[col].value_counts(dropna=False)\n    print(f\"\\nüî¢ Total Unique Values in '{col}': {df_train[col].nunique(dropna=False)}\")\n    print(\"=\" * 50)","metadata":{"trusted":true,"id":"ai_5Qrs0VJo8","outputId":"a1f8f29b-8115-455a-898b-2d4bea8bbdc5","execution":{"iopub.status.busy":"2025-04-25T21:24:57.399342Z","iopub.execute_input":"2025-04-25T21:24:57.399633Z","iopub.status.idle":"2025-04-25T21:24:57.430150Z","shell.execute_reply.started":"2025-04-25T21:24:57.399614Z","shell.execute_reply":"2025-04-25T21:24:57.429341Z"}},"outputs":[{"name":"stdout","text":"\nüî¢ Total Unique Values in 'STUS_CD': 5\n==================================================\n\nüî¢ Total Unique Values in 'TYPE_ADM': 5\n==================================================\n\nüî¢ Total Unique Values in 'SRC_ADMS': 3\n==================================================\n\nüî¢ Total Unique Values in 'AD_DGNS': 2236\n==================================================\n\nüî¢ Total Unique Values in 'stay_drg_cd': 519\n==================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#converting into categorical columns\ncategorical_cols = ['STUS_CD', 'TYPE_ADM', 'SRC_ADMS']\ndf_train[categorical_cols] = df_train[categorical_cols].astype('category')","metadata":{"trusted":true,"id":"-Qdh6ULEVJo9","execution":{"iopub.status.busy":"2025-04-25T21:25:03.244371Z","iopub.execute_input":"2025-04-25T21:25:03.244687Z","iopub.status.idle":"2025-04-25T21:25:03.257618Z","shell.execute_reply.started":"2025-04-25T21:25:03.244667Z","shell.execute_reply":"2025-04-25T21:25:03.256693Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df_train.dtypes","metadata":{"trusted":true,"id":"UicHZrXgVJo9","outputId":"6b44d053-e27f-45d5-9ed2-b1574f90513e","execution":{"iopub.status.busy":"2025-04-25T21:25:12.259496Z","iopub.execute_input":"2025-04-25T21:25:12.259775Z","iopub.status.idle":"2025-04-25T21:25:12.267432Z","shell.execute_reply.started":"2025-04-25T21:25:12.259756Z","shell.execute_reply":"2025-04-25T21:25:12.266679Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"STUS_CD          category\nTYPE_ADM         category\nSRC_ADMS         category\nAD_DGNS            object\nDGNSCD01           object\nPRCDRCD01          object\nDGNSCD02           object\nPRCDRCD02          object\nDGNSCD03           object\nPRCDRCD03          object\nDGNSCD04           object\nPRCDRCD04          object\nDGNSCD05           object\nPRCDRCD05          object\nDGNSCD06           object\nPRCDRCD06          object\nDGNSCD07           object\nPRCDRCD07          object\nDGNSCD08           object\nPRCDRCD08          object\nDGNSCD09           object\nPRCDRCD09          object\nDGNSCD10           object\nPRCDRCD10          object\nDGNSCD11           object\nPRCDRCD11          object\nDGNSCD12           object\nPRCDRCD12          object\nDGNSCD13           object\nPRCDRCD13          object\nDGNSCD14           object\nPRCDRCD14          object\nDGNSCD15           object\nPRCDRCD15          object\nDGNSCD16           object\nPRCDRCD16          object\nDGNSCD17           object\nPRCDRCD17          object\nDGNSCD18           object\nPRCDRCD18          object\nDGNSCD19           object\nPRCDRCD19          object\nDGNSCD20           object\nPRCDRCD20          object\nDGNSCD21           object\nPRCDRCD21          object\nDGNSCD22           object\nPRCDRCD22          object\nDGNSCD23           object\nPRCDRCD23          object\nDGNSCD24           object\nPRCDRCD24          object\nDGNSCD25           object\nPRCDRCD25          object\nstay_drg_cd         Int64\nReadmitted_30       int64\ndtype: object"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"---\n\n# üìà Data Analysis & EDA & Features Selection","metadata":{"id":"WY-cYbp4VJo9"}},{"cell_type":"markdown","source":"# Hidden Layers Baseline Model\n\nThis baseline model is used to preprocess the training data, train a classifier, evaluate its performance, and plot feature importance. It can also be used for feature selection based on feature importance and the performance metrics.\n\n## Function Overview\nThe `hidden_layers_baseline` function performs the following tasks:\n1. **Preprocessing**:\n   - Drops unnecessary columns.\n   - Handles missing values by filling or dropping them.\n   - Encodes categorical features (e.g., `stay_drg_cd`).\n   \n2. **Model Training**:\n   - Splits the dataset into training and validation sets.\n   - Trains a CatBoost Classifier using GPU acceleration and class weights.\n   \n3. **Model Evaluation**:\n   - Predicts on the validation set and computes the following metrics:\n     - F1 Score\n     - AUC-ROC\n     - Precision\n     - Recall\n     \n4. **Plots**:\n   - Bar chart of evaluation metrics.\n   - ROC Curve to evaluate the model's true positive vs false positive rates.\n\n5. **Feature Importance**:\n   - Extracts feature importance scores from the trained model.\n   - Plots the top 20 most important features based on the CatBoost model.\n\n6. **Test Data Prediction**:\n   - Prepares the test dataset in the same way as the training data.\n   - Makes predictions on the test set and saves the results in a CSV file.\n\n## Detailed Steps\n\n### 1. **Data Preprocessing**\n   The preprocessing steps include:\n   - Dropping irrelevant columns such as `ID`, `STAY_DRG_CD`, `STAY_FROM_DT`, and `STAY_THRU_DT`.\n   - Dropping rows where `stay_drg_cd` is missing.\n   - Encoding the `stay_drg_cd` column as numeric values.\n   - Filling missing values with `'unknown'` for categorical features.\n\n### 2. **Model Training**\n   The CatBoost classifier is used to train the model:\n   - It uses GPU acceleration for faster training (`task_type=\"GPU\"`).\n   - Class weights are adjusted to deal with class imbalances (class weights: `[0.63, 2.44]`).\n   - The model is trained on the `X_train` and `y_train` datasets, with categorical features specified.\n\n### 3. **Model Evaluation**\n   After training, the model's performance is evaluated on the validation set (`X_val`, `y_val`) using several metrics:\n   - **F1 Score**: Balances precision and recall for binary classification tasks.\n   - **AUC-ROC**: Measures the trade-off between true positive rate and false positive rate.\n   - **Precision**: Measures the proportion of positive predictions that are actually correct.\n   - **Recall**: Measures the proportion of actual positives that are correctly predicted.\n\n   The results are visualized using a bar chart with the metrics displayed for easy comparison.\n\n### 4. **ROC Curve**\n   The ROC curve is plotted to visually assess the performance of the classifier. The area under the curve (AUC) is also calculated, which indicates the ability of the model to discriminate between classes.\n\n### 5. **Feature Importance**\n   The feature importance is obtained from the CatBoost model, which helps identify the most influential features in making predictions:\n   - The importance of each feature is printed and saved to a CSV file (`feature_importance.csv`).\n   - A horizontal bar chart is plotted to visualize the top 20 most important features.\n\n### 6. **Test Data Prediction**\n   After preprocessing the test data (similar to the training data), predictions are made on the test set, and the results are saved to a submission file (`submission.csv`).\n\n### Example of Usage:\n```python\nhidden_layers_baseline('train_data.csv', 'test_data.csv', submission_filename='submission.csv')","metadata":{}},{"cell_type":"code","source":"def hidden_layers_baseline(train_df_path, test_df_path, submission_filename='submission.csv'):\n    df = pd.read_csv(train_df_path)\n\n    # Preprocessing\n    df.drop(columns=['ID','STAY_DRG_CD','STAY_FROM_DT','STAY_THRU_DT'], inplace=True)\n    df.dropna(subset=['stay_drg_cd'], inplace=True)\n    df['stay_drg_cd'] = pd.to_numeric(df['stay_drg_cd'], errors='coerce').astype('Int64')\n    df.dropna(subset=['stay_drg_cd'], inplace=True)\n    df.fillna('unknown', inplace=True)\n\n    X = df.drop(columns=['Readmitted_30'])\n    y = df['Readmitted_30']\n    cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n\n    X_train, X_val, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n    # Train model\n    model = CatBoostClassifier(\n        #task_type=\"GPU\",\n        #devices='0',\n        verbose=100,\n        random_state=42,\n        class_weights=[0.63, 2.44]\n    )\n    model.fit(X_train, y_train, cat_features=cat_features)\n\n    # Predict & Metrics\n    y_pred = model.predict(X_val)\n    y_prob = model.predict_proba(X_val)[:, 1]\n    f1 = f1_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_prob)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n\n    # Plotting metric bar chart\n    plt.figure(figsize=(8, 5))\n    metrics = {'F1 Score': f1, 'AUC-ROC': auc_score, 'Precision': precision, 'Recall': recall}\n    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='viridis')\n    plt.ylim(0, 1)\n    plt.title(\"Model Performance Metrics\")\n    for i, value in enumerate(metrics.values()):\n        plt.text(i, value + 0.02, f\"{value:.2f}\", ha='center', va='bottom', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n    # Plotting ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    roc_auc = sk_auc(fpr, tpr)\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2, label='Random')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Recall)')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.show()\n\n    # Feature Importances\n    feature_importances = model.get_feature_importance()\n    feature_names = X_train.columns\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': feature_importances\n    }).sort_values(by='Importance', ascending=False)\n    print(importance_df)\n    importance_df.to_csv('feature_importance.csv')\n\n    plt.figure(figsize=(12, 8))\n    plt.barh(importance_df['Feature'][::-1], importance_df['Importance'][::-1], color='skyblue')\n    plt.xlabel('Importance')\n    plt.title('Top 20 Feature Importances (CatBoost)')\n    plt.grid(axis='x')\n    plt.tight_layout()\n    plt.show()\n\n    # Test data preprocessing\n    test_df = pd.read_csv(test_df_path)\n    submission_ids = test_df['ID']\n    test_df.drop(columns=['ID', 'STAY_DRG_CD', 'STAY_FROM_DT', 'STAY_THRU_DT'], inplace=True)\n    test_df['stay_drg_cd'] = pd.to_numeric(test_df['stay_drg_cd'], errors='coerce').fillna(-1).astype('Int64')\n    test_df.fillna('unknown', inplace=True)\n\n    # Predict on test\n    y_pred_test = model.predict(test_df)\n\n    # Submission\n    submission_df = pd.DataFrame({\n        'ID': submission_ids,\n        'Readmitted_30': y_pred_test\n    })\n    submission_df.to_csv(submission_filename, index=False)\n    print(f\"Submission file saved as {submission_filename}\")","metadata":{"trusted":true,"id":"rHSPWzDkVJpA","execution":{"iopub.status.busy":"2025-04-25T21:27:14.255339Z","iopub.execute_input":"2025-04-25T21:27:14.255805Z","iopub.status.idle":"2025-04-25T21:27:14.279906Z","shell.execute_reply.started":"2025-04-25T21:27:14.255771Z","shell.execute_reply":"2025-04-25T21:27:14.278988Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"hidden_layers_baseline(\n    train_df_path= '/kaggle/input/softec-25-machine-learning-competition/train.csv',\n    test_df_path='/kaggle/input/softec-25-machine-learning-competition/test.csv',\n    submission_filename='submission_2.csv'\n)","metadata":{"trusted":true,"id":"OAkbIHWYVJpB","outputId":"8bbfdf19-5309-4588-eb5f-4c03527b69c4","execution":{"iopub.status.busy":"2025-04-25T21:27:15.196957Z","iopub.execute_input":"2025-04-25T21:27:15.197368Z","execution_failed":"2025-04-25T21:28:00.494Z"}},"outputs":[{"name":"stdout","text":"Learning rate set to 0.072884\n0:\tlearn: 0.6905125\ttotal: 619ms\tremaining: 10m 18s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Baseline Model Results\n\nThe results from the baseline model suggest that the most relevant features for predicting readmission (label `Readmitted_30`) have been identified. Based on feature importance and model evaluation, the following columns were selected for training and testing:\n\n- **Columns to Keep**: These columns will be used for model training and include various diagnosis, procedure codes, and admission types.\n- **Columns to Test**: These columns will be used for making predictions on the test dataset, with a similar set of features as the training set.\n\n","metadata":{}},{"cell_type":"code","source":"# Columns to keep\ncolumns_to_keep = [\n    'STUS_CD', 'stay_drg_cd', 'DGNSCD02', 'DGNSCD01', 'PRCDRCD01', 'AD_DGNS', 'DGNSCD18', 'DGNSCD03',\n    'DGNSCD17', 'TYPE_ADM', 'DGNSCD05', 'DGNSCD08', 'DGNSCD06', 'DGNSCD07', 'DGNSCD13', 'DGNSCD04',\n    'DGNSCD12', 'DGNSCD11', 'DGNSCD15', 'DGNSCD09', 'DGNSCD14', 'DGNSCD16', 'DGNSCD10', 'DGNSCD24',\n    'PRCDRCD02', 'DGNSCD19', 'DGNSCD20', 'DGNSCD22', 'DGNSCD25', 'PRCDRCD03', 'DGNSCD21', 'SRC_ADMS',\n    'PRCDRCD04', 'DGNSCD23', 'PRCDRCD05', 'PRCDRCD06', 'PRCDRCD22', 'PRCDRCD10', 'PRCDRCD09',\n    'PRCDRCD21', 'PRCDRCD11', 'PRCDRCD07', 'PRCDRCD08', 'PRCDRCD19', 'PRCDRCD12', 'PRCDRCD13', 'PRCDRCD15', 'Readmitted_30'\n]\ncolumns_to_test = [\n    'STUS_CD', 'stay_drg_cd', 'DGNSCD02', 'DGNSCD01', 'PRCDRCD01', 'AD_DGNS', 'DGNSCD18', 'DGNSCD03',\n    'DGNSCD17', 'TYPE_ADM', 'DGNSCD05', 'DGNSCD08', 'DGNSCD06', 'DGNSCD07', 'DGNSCD13', 'DGNSCD04',\n    'DGNSCD12', 'DGNSCD11', 'DGNSCD15', 'DGNSCD09', 'DGNSCD14', 'DGNSCD16', 'DGNSCD10', 'DGNSCD24',\n    'PRCDRCD02', 'DGNSCD19', 'DGNSCD20', 'DGNSCD22', 'DGNSCD25', 'PRCDRCD03', 'DGNSCD21', 'SRC_ADMS',\n    'PRCDRCD04', 'DGNSCD23', 'PRCDRCD05', 'PRCDRCD06', 'PRCDRCD22', 'PRCDRCD10', 'PRCDRCD09',\n    'PRCDRCD21', 'PRCDRCD11', 'PRCDRCD07', 'PRCDRCD08', 'PRCDRCD19', 'PRCDRCD12', 'PRCDRCD13', 'PRCDRCD15'\n]\ndf_train = df_train[columns_to_keep]\ndf_test = df_test[columns_to_test]","metadata":{"trusted":true,"id":"v_xE84zDVJpH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting Training Data by Target Value\n\nThe training data is split into two subsets based on the target variable `Readmitted_30`, which indicates whether the patient was readmitted within 30 days:\n\n- **df_target_0**: Contains rows where `Readmitted_30` is 0 (no readmission).\n- **df_target_1**: Contains rows where `Readmitted_30` is 1 (readmitted within 30 days).","metadata":{}},{"cell_type":"code","source":"df_target_0 = df_train[df_train['Readmitted_30'] == 0].copy()\ndf_target_1 = df_train[df_train['Readmitted_30'] == 1].copy()\nprint(f'Total Train Data Shape',df_train.shape)\nprint(f'Total Train Data Shape with target column 0',df_target_0.shape)\nprint(f'Total Train Data Shape with target column 1',df_target_1.shape)\nprint(f'Total Test Data Shape with test',df_test.shape)","metadata":{"trusted":true,"id":"jEejNs5PVJpI","outputId":"2848d056-8d35-4640-b163-d509e6bdb58e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outlier Analysis and Distribution of `STUS_CD` for Target Classes\n\nIn this analysis, we examine the distribution of the `STUS_CD` feature for two target classes in the dataset: \n\n- **Readmitted_30 = 0** (patients not readmitted within 30 days)\n- **Readmitted_30 = 1** (patients readmitted within 30 days)\n\n#### Steps:\n1. **Outlier Detection**: \n   - The Interquartile Range (IQR) method is used to detect outliers. Any values outside the range defined by `Q1 - 1.5 * IQR` and `Q3 + 1.5 * IQR` are considered outliers.\n   - We calculate the number and percentage of outliers for both target classes.\n\n2. **Boxplot**:\n   - A boxplot is plotted for each target class to visualize the spread and presence of outliers.\n\n3. **Distribution Plot with KDE**:\n   - A histogram and Kernel Density Estimation (KDE) curve are plotted for both target classes to understand the underlying distribution of `STUS_CD`.\n\n#### Results:\nFor **Readmitted_30 = 0**:\n- **Number of outliers**: Printed after analysis.\n- **Percentage of outliers**: Printed after analysis.\n  \nFor **Readmitted_30 = 1**:\n- **Number of outliers**: Printed after analysis.\n- **Percentage of outliers**: Printed after analysis.\n\nThis approach provides insights into the variability and distribution of the feature across different target classes.\n\n\n","metadata":{}},{"cell_type":"code","source":"data = pd.to_numeric(df_target_0['STUS_CD'], errors='coerce').dropna()\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = data[(data < lower_bound) | (data > upper_bound)]\nnum_outliers = outliers.count()\npercent_outliers = (num_outliers / len(data)) * 100\n\n\nplt.figure(figsize=(8, 2))\nplt.boxplot(data, vert=False)\nplt.title('STUS_CD Boxplot (Readmitted_30 = 0)')\nplt.xlabel('STUS_CD')\nplt.show()\nprint(f\"[Target = 0] Number of outliers: {num_outliers}\")\nprint(f\"[Target = 0] Percentage of outliers: {percent_outliers:.2f}%\")\n\nplt.figure(figsize=(8, 4))\nax = data.plot.hist(bins=30, density=True, alpha=0.6)\ndata.plot.kde(ax=ax)\nplt.title('STUS_CD Distribution with KDE (Readmitted_30 = 0)')\nplt.xlabel('STUS_CD')\nplt.ylabel('Density')\nplt.show()\nprint('-'*40)\ndata1 = pd.to_numeric(df_target_1['STUS_CD'], errors='coerce').dropna()\nQ1 = data1.quantile(0.25)\nQ3 = data1.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = data1[(data1 < lower_bound) | (data1 > upper_bound)]\nnum_outliers = outliers.count()\npercent_outliers = (num_outliers / len(data1)) * 100\nplt.figure(figsize=(8, 2))\nplt.boxplot(data1, vert=False)\nplt.title('STUS_CD Boxplot (Readmitted_30 = 1)')\nplt.xlabel('STUS_CD')\nplt.show()\nprint(f\"[Target = 1] Number of outliers: {num_outliers}\")\nprint(f\"[Target = 1] Percentage of outliers: {percent_outliers:.2f}%\")\nplt.figure(figsize=(8, 4))\nax = data1.plot.hist(bins=30, density=True, alpha=0.6)\ndata1.plot.kde(ax=ax)\nplt.title('STUS_CD Distribution with KDE (Readmitted_30 = 1)')\nplt.xlabel('STUS_CD')\nplt.ylabel('Density')\nplt.show()","metadata":{"trusted":true,"id":"9M3qo_HDVJpI","outputId":"9dca6b3d-576e-4dba-e085-b70ff9e9a0f5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outlier Handling: Winsorization for Target 0 and Capping for Target 1\n\nIn this analysis, we handle the outliers in the `STUS_CD` feature for two target classes (Readmitted_30 = 0 and Readmitted_30 = 1) in the dataset:\n\n1. **For Target 0 (Readmitted_30 = 0)**:\n   - **Outlier Removal**: We remove the rows where the `STUS_CD` values fall outside the calculated bounds using the IQR method (lower and upper bounds). \n   - **Result**: Rows with extreme `STUS_CD` values are removed, reducing the dataset size for this target class.\n\n2. **For Target 1 (Readmitted_30 = 1)**:\n   - **Outlier Capping (Winsorization)**: Instead of removing outliers, we cap the extreme `STUS_CD` values that are below the lower bound or above the upper bound to the respective boundary value (Winsorization).\n   - **Result**: Outliers are transformed, ensuring no rows are removed from this dataset.\n\nAfter cleaning, the following insights are provided:\n- The number and percentage of rows removed from Target 0 due to outliers.\n- The number of outliers capped in Target 1 without any rows being removed.\n- The distribution and outliers are visualized using boxplots and histograms with Kernel Density Estimation (KDE) for both target classes.\n\nThis method ensures that we handle outliers appropriately while maintaining the integrity of the dataset for model training.\n","metadata":{}},{"cell_type":"code","source":"#winsorize 0 target outlier\ndf_target_0_cleaned = df_target_0[\n    (pd.to_numeric(df_target_0['STUS_CD'], errors='coerce') >= lower_bound) &\n    (pd.to_numeric(df_target_0['STUS_CD'], errors='coerce') <= upper_bound)\n]\ndf_target_1_cleaned = df_target_1.copy()\ndf_target_1_cleaned['STUS_CD'] = pd.to_numeric(df_target_1_cleaned['STUS_CD'], errors='coerce')\n\n# Capping outliers with target 1\ndf_target_1_cleaned['STUS_CD'] = df_target_1_cleaned['STUS_CD'].clip(lower=lower_bound, upper=upper_bound)","metadata":{"trusted":true,"id":"UeWzbJaLVJpJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Total Train Data Shape with target column 0: {df_target_0.shape}')\nprint(f'Total Train Data Shape with target column 1: {df_target_1.shape}')\nprint('\\nAfter Removing Outliers:\\n')\nprint(f'Total Train Data Shape with target column 0: {df_target_0_cleaned.shape}')\nprint(f'Total Train Data Shape with target column 1: {df_target_1_cleaned.shape}')\nprint('\\n')\n\n# For target 0\nremoved_rows_target_0 = df_target_0.shape[0] - df_target_0_cleaned.shape[0]\nprint(f'Rows removed from target 0 dataset due to outliers: {removed_rows_target_0} ({(removed_rows_target_0 / df_target_0.shape[0]) * 100:.2f}%)')\n\n# For target 1\nremoved_rows_target_1 = df_target_1.shape[0] - df_target_1_cleaned.shape[0]\nprint(f'Rows capped in target 1 dataset (not removed, just transformed): {removed_rows_target_1} (0.00%)')","metadata":{"trusted":true,"id":"LHEwB9DyVJpK","outputId":"6ed13cd4-9485-4df1-f54b-bba75d0380d2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === For df_target_1 ===\ndata_1 = pd.to_numeric(df_target_1_cleaned['STUS_CD'], errors='coerce').dropna()\n\n# Compute IQR-based outliers\nQ1 = data_1.quantile(0.25)\nQ3 = data_1.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = data1[(data_1 < lower_bound) | (data_1 > upper_bound)]\nnum_outliers = outliers.count()\npercent_outliers = (num_outliers / len(data_1)) * 100\n\n# Boxplot\nplt.figure(figsize=(8, 2))\nplt.boxplot(data_1, vert=False)\nplt.title('STUS_CD Boxplot (Readmitted_30 = 1)')\nplt.xlabel('STUS_CD')\nplt.show()\n\nprint(f\"[Target = 1] Number of outliers: {num_outliers}\")\nprint(f\"[Target = 1] Percentage of outliers: {percent_outliers:.2f}%\")\n\n# Distribution with KDE\nplt.figure(figsize=(8, 4))\nax = data_1.plot.hist(bins=30, density=True, alpha=0.6)\ndata1.plot.kde(ax=ax)\nplt.title('STUS_CD Distribution with KDE (Readmitted_30 = 1)')\nplt.xlabel('STUS_CD')\nplt.ylabel('Density')\nplt.show()","metadata":{"trusted":true,"id":"BLb7EwDXVJpK","outputId":"b20ab98b-61d8-4227-9843-e57da1e5804c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === For df_test ===\ndata = pd.to_numeric(df_test['STUS_CD'], errors='coerce').dropna()\n\n# Compute IQR-based outliers\nQ1 = data.quantile(0.25)\nQ3 = data.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = data[(data < lower_bound) | (data > upper_bound)]\nnum_outliers = outliers.count()\npercent_outliers = (num_outliers / len(data)) * 100\n\n# Boxplot\nplt.figure(figsize=(8, 2))\nplt.boxplot(data, vert=False)\nplt.title('STUS_CD Boxplot (Readmitted_30 = 0)')\nplt.xlabel('STUS_CD')\nplt.show()\n\nprint(f\"[Target = 0] Number of outliers: {num_outliers}\")\nprint(f\"[Target = 0] Percentage of outliers: {percent_outliers:.2f}%\")","metadata":{"trusted":true,"id":"qDExun-uVJpL","outputId":"f399a82d-c990-4b65-f7f4-873be6c82e6a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 6: Analyze Missing Value Ranges in `df_target_0`\n\n- We calculate the percentage of missing values for each row in `df_target_0`.\n- We then analyze rows that have more than 50%, 60%, 70%, 80%, and 90% missing values. For each threshold, we calculate the percentage and number of rows with missing values exceeding the threshold.\n\n### Step 7: Analyze Missing Value Ranges in `df_target_1`\n\n- The same analysis is performed on `df_target_1` to determine the rows with varying percentages of missing values.\n\n### Dropping Rows with Missing Values Above a Threshold\n\n- Rows with more than a specified threshold (e.g., 70%) of missing values are dropped to clean the dataset.\n- This reduces the dataset size by removing rows with excessive missing values, ensuring that only rows with sufficient data are retained.\n\n### Imputing Missing Values\n\n- For remaining missing values:\n  - **Categorical columns** are filled with the mode (most frequent value).\n  - **Numerical columns** are filled using the median, ensuring the values fall within the Interquartile Range (IQR) bounds.\n\n- The imputation process ensures that the dataset has no missing values after cleaning.\n\n### Final Dataset\n\n- After dropping rows and imputing missing values, the final cleaned datasets for both training (`df_target_0` and `df_target_1`) and test (`df_test`) are obtained, and their shapes are displayed for verification.\n","metadata":{}},{"cell_type":"code","source":"# Analyzing Missing Value Ranges in df_target_0\nmissing_percentage = df_target_0.isnull().mean(axis=1)\n\nthresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n\nfor thresh in thresholds:\n    percent = (missing_percentage > thresh).mean() * 100\n    print(f\"Rows with >{int(thresh*100)}% missing values: {percent:.2f}% ({(missing_percentage > thresh).sum()} rows)\")","metadata":{"trusted":true,"id":"tQm_IfC0VJpL","outputId":"d32e13fe-0529-4153-9f7a-fb77a2aef74a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyzing Missing Value Ranges in df_target_1\nmissing_percentage = df_target_1.isnull().mean(axis=1)\n\nthresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n\nfor thresh in thresholds:\n    percent = (missing_percentage > thresh).mean() * 100\n    print(f\"Rows with >{int(thresh*100)}% missing values: {percent:.2f}% ({(missing_percentage > thresh).sum()} rows)\")","metadata":{"trusted":true,"id":"o6WAahWGVJpL","outputId":"995c2eb8-b2f6-41df-c2ad-ba07b80b448c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def drop_rows_with_missing_threshold(df, threshold_percent=70):\n    \"\"\"\n    Drops rows from the DataFrame that have more than `threshold_percent` of missing values.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        threshold_percent (float): Threshold percentage (0-100) for missing values.\n\n    Returns:\n        pd.DataFrame: Cleaned DataFrame.\n    \"\"\"\n    # Calculate how many columns is the threshold\n    threshold_count = int((threshold_percent / 100.0) * df.shape[1])\n\n    # Drop rows with more than threshold_count missing values\n    df_cleaned = df[df.isnull().sum(axis=1) <= threshold_count].copy()\n\n    print(f\"‚úÖ Rows before: {df.shape[0]}, after: {df_cleaned.shape[0]} (Dropped {df.shape[0] - df_cleaned.shape[0]})\")\n    return df_cleaned","metadata":{"trusted":true,"id":"wWHBGjUPVJpM"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_target_0 = drop_rows_with_missing_threshold(df_target_0, threshold_percent=70)","metadata":{"trusted":true,"id":"3p1isIMLVJpM","outputId":"900efd9c-d0f5-4ba5-db39-270ac96b350c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def impute_missing_values(df):\n    \"\"\"\n    Imputes missing values:\n      - For categorical columns: fills with mode.\n      - For numerical columns: fills using IQR-based strategy (median +/- 1.5*IQR).\n\n    Returns:\n        pd.DataFrame: DataFrame with imputed values.\n    \"\"\"\n    df_imputed = df.copy()\n\n    for col in df_imputed.columns:\n        if df_imputed[col].dtype == 'object' or df_imputed[col].dtype.name == 'category':\n            # Fill categorical with mode\n            mode = df_imputed[col].mode()\n            if not mode.empty:\n                df_imputed[col].fillna(mode[0], inplace=True)\n        else:\n            # For numeric columns\n            Q1 = df_imputed[col].quantile(0.25)\n            Q3 = df_imputed[col].quantile(0.75)\n            IQR = Q3 - Q1\n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            median = df_imputed[col].median()\n\n            # Fill missing values with median if within IQR bounds, else just use median\n            df_imputed[col].fillna(median, inplace=True)\n\n    print(\"‚úÖ Null values imputed successfully.\")\n    return df_imputed","metadata":{"trusted":true,"id":"k8Zl52UVVJpM"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_target_0 = impute_missing_values(df_target_0)\ndf_target_1 = impute_missing_values(df_target_1)\ndf_test = impute_missing_values(df_test)","metadata":{"trusted":true,"id":"XIYgMFCuVJpN","outputId":"85454a12-c1c8-4818-8e2a-81faf7bea5e2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Missing values in df_target_0 : {df_target_0.isnull().sum().sum()}')\nprint(f'Missing values in df_target_1 : {df_target_1.isnull().sum().sum()}')\nprint(f'Missing values in df_test : {df_test.isnull().sum().sum()}')","metadata":{"trusted":true,"id":"9a1GYhndVJpN","outputId":"671d95e8-3ecf-4416-f58c-48bba4c075d4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_cleaned = pd.concat([df_target_0, df_target_1], ignore_index=True)","metadata":{"trusted":true,"id":"xnE0GYbUVJpN"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Cleaned train Dataset shape :{df_cleaned.shape}')\nprint(f'Cleaned test Dataset shape :{df_test.shape}')","metadata":{"trusted":true,"id":"YiDZAPc6VJpO","outputId":"58f533af-14c8-4759-b6dc-5e19ab349a2e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_column_mismatches(df_train, df_test):\n    \"\"\"\n    Prints the mismatched columns between train and test DataFrames without dropping any columns.\n\n    Parameters:\n        df_train (pd.DataFrame): Training dataset\n        df_test (pd.DataFrame): Testing dataset\n    \"\"\"\n    train_cols = set(df_train.columns)\n    test_cols = set(df_test.columns)\n\n    train_extra = train_cols - test_cols\n    test_extra = test_cols - train_cols\n\n    if train_extra or test_extra:\n        print(\"‚ö†Ô∏è Column mismatch found:\")\n        if train_extra:\n            print(f\"üü† Columns in train but not in test: {sorted(train_extra)}\")\n        if test_extra:\n            print(f\"üîµ Columns in test but not in train: {sorted(test_extra)}\")\n    else:\n        print(\"‚úÖ Train and test datasets have exactly the same columns.\")\n\n    print(f\"\\nTrain Dataset shape : {df_train.shape}\")\n    print(f\"Test Dataset shape  : {df_test.shape}\")","metadata":{"trusted":true,"id":"BYDTM1BDVJpO"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_column_mismatches(df_cleaned, df_test)","metadata":{"trusted":true,"id":"B9D-uGqLVJpO","outputId":"089d2374-68e9-4557-98dd-60c435c7bf91"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# üß† Model Selection","metadata":{}},{"cell_type":"markdown","source":" \n\n---\n\n# ‚öôÔ∏è Why Use CatBoost?\n\n- CatBoost is a gradient boosting library developed by Yandex, tailored for categorical features.  \n- Handles categorical variables **natively** ‚Äî no need for manual encoding like One-Hot or Label Encoding.  \n- Supports **GPU training**, significantly speeding up experiments.  \n- Includes **regularization parameters** and **early stopping**, making it highly resistant to overfitting.  \n- Delivers **state-of-the-art performance** on tabular data with minimal preprocessing.  \n\n---\n\n  \n\n---\n\n# üßæ Explained: Key Hyperparameters Tuned via Optuna  \n\n- `iterations`: Number of boosting rounds. Higher = potentially better fit but more risk of overfitting.  \n- `learning_rate`: Controls step size in gradient descent. Lower = slower but more precise learning.  \n- `depth`: Tree depth. Shallow trees overfit less; deeper trees can learn more complex patterns.  \n- `l2_leaf_reg`: L2 regularization term to **penalize overly complex models**.  \n- `random_strength`: Adds randomness in tree splitting to improve generalization.  \n- `bootstrap_type`: Sampling strategy for boosting:  \n  - **Bayesian**: Adds randomness using Bayesian bootstrapping.  \n  - **Bernoulli**: Classic random subsampling.  \n- `subsample`: Only applies to Bernoulli. Reduces overfitting by training on random subsets.  \n- `early_stopping_rounds`: Stops training if no improvement seen in N rounds ‚Äî a great overfitting guard.  \n\n---\n\n# ‚öñÔ∏è Why Use `class_weights`?\n\n- Addresses **class imbalance** in the target (`Readmitted_30`).  \n- Heavier weight is assigned to the **minority class** to reduce bias toward majority.  \n\n### üìê Formula for Balanced Class Weights:\n\n\\[\n\\text{weight}_i = \\frac{n_{\\text{samples}}}{n_{\\text{classes}} \\times n_{\\text{samples}_i}}\n\\]\n\n- In our case: `class_weights = [0.55, 2.0]`  \n  - This means the second class (likely \"readmitted\") is ~4x rarer than the first.  \n\n---\n\n# üéØ Step 3: Running the Optuna Study  \n\n- Optuna searches the parameter space to **maximize AUC**, ideal for imbalanced classification.  \n- Even though only one trial was run here, typically **dozens to hundreds** are used.  \n\nExtracted after tuning:\n- `study.best_value`: Best AUC achieved.  \n- `study.best_params`: Parameters that led to best performance.  \n\n---\n\n# üß† Step 4: Final Model Training Using Best Parameters  \n\n- Used the best hyperparameters from Optuna to train on the **full cleaned dataset (X, y)**.  \n- Ensured model used **optimal regularization, structure, and learning rate**.  \n- Enabled `task_type = \"GPU\"` to **speed up training**.  \n\n---\n\n# üì¶ Step 5: Predictions and Submission File  \n\n- Preprocessed the test dataset using the same feature order as training data.  \n- Used the trained model to make predictions.  \n- Created `submission_optuna.csv` containing:  \n  - `ID`: From test set.  \n  - `Readmitted_30`: Predicted label.  \n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_cleaned.drop(columns=['PRCDRCD19','PRCDRCD13','PRCDRCD11','PRCDRCD22'],inplace=True)\ndf_cleaned.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üö´ Removal of Leakage-Prone Features  \nWe dropped features: `PRCDRCD19`, `PRCDRCD13`, `PRCDRCD11`, and `PRCDRCD22`.  \n\n- These were found to cause data leakage, leading to suspiciously high validation performance and poor generalization.  \n- Including such features would mean the model \"cheats\" by using information not available at prediction time.  \n- ‚úÖ Removing them improved model robustness and prevented overfitting. ","metadata":{}},{"cell_type":"code","source":"X = df_cleaned.drop(\"Readmitted_30\", axis=1)\ny = df_cleaned[\"Readmitted_30\"]\n\ncat_features = X.select_dtypes(include=['category', 'object']).columns.tolist()\n\ndef objective(trial):\n    bootstrap_type = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"])\n    \n    params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 500, 1500),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 1.0, 10.0),\n        \"bootstrap_type\": bootstrap_type,\n        \"class_weights\": [0.55, 2.0],\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"AUC\",\n        \"verbose\": 0,\n        \"task_type\": \"GPU\",\n        \"devices\": \"0\",\n        \"early_stopping_rounds\": 50,\n        \"random_seed\": 42\n    }\n\n    # subsample for Bernoulli bootstrapping\n    if bootstrap_type == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.6, 1.0)\n\n    auc_scores = []\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = CatBoostClassifier(**params)\n        model.fit(\n            X_train, y_train,\n            eval_set=(X_val, y_val),\n            use_best_model=True,\n            cat_features=cat_features\n        )\n\n        preds = model.predict_proba(X_val)[:, 1]\n        auc = roc_auc_score(y_val, preds)\n        auc_scores.append(auc)\n\n    return np.mean(auc_scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß™ Step 1: Preparing Data for Optuna Study  \n\n- Separated features (`X`) from the target (`y = Readmitted_30`).  \n- Identified **categorical columns** to inform CatBoost which features require special handling.  \n- Preparing this before training ensures **reproducibility** and proper model configuration.  \n\n---\n\n## üõ†Ô∏è Step 2: Optuna Objective Function for Hyperparameter Tuning  \n\n- Used **Optuna**, a powerful library for automated hyperparameter optimization.  \n- Defined a custom `objective()` function that:\n  - Performs **5-fold Stratified Cross-Validation** to maintain class distribution.  \n  - Trains `CatBoostClassifier` using trial-specific parameters.  \n  - Returns the **mean AUC** across folds for each trial.\n\n---\n\n## üßæ Explained: Key Hyperparameters Tuned via Optuna  \n\n- `iterations`: Number of boosting rounds. Higher = potentially better fit but more risk of overfitting.  \n- `learning_rate`: Controls step size in gradient descent. Lower = slower but more precise learning.  \n- `depth`: Tree depth. Shallow trees overfit less; deeper trees can learn more complex patterns.  \n- `l2_leaf_reg`: L2 regularization term to **penalize overly complex models**.  \n- `random_strength`: Adds randomness in tree splitting to improve generalization.  \n- `bootstrap_type`: Sampling strategy for boosting:  \n  - **Bayesian**: Adds randomness using Bayesian bootstrapping.  \n  - **Bernoulli**: Classic random subsampling.  \n- `subsample`: Only applies to Bernoulli. Reduces overfitting by training on random subsets.  \n- `early_stopping_rounds`: Stops training if no improvement seen in N rounds ‚Äî a great overfitting guard.  \n\n---\n\n## ‚öñÔ∏è Why Use `class_weights`?\n\n- Addresses **class imbalance** in the target (`Readmitted_30`).  \n- Heavier weight is assigned to the **minority class** to reduce bias toward majority.  \n\n### üìê Formula for Balanced Class Weights:\n\n\\[\n\\text{weight}_i = \\frac{n_{\\text{samples}}}{n_{\\text{classes}} \\times n_{\\text{samples}_i}}\n\\]\n\n\n- In our case: `class_weights = [0.55, 2.0]`  \n- This means the second class (likely \"readmitted\") is ~4x rarer than the first.  \n\n---","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=1)\nprint(\"Best AUC:\", study.best_value)\nprint(\"Best Params:\", study.best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üéØ Step 3: Running the Optuna Study  \n\n- Optuna searches the parameter space to **maximize AUC**, ideal for imbalanced classification.  \n- Even though only one trial was run here, typically **dozens to hundreds** are used.  \n\nExtracted after tuning:\n- `study.best_value`: Best AUC achieved.  \n- `study.best_params`: Parameters that led to best performance.  ","metadata":{}},{"cell_type":"code","source":"# Grabing the best params from Optuna study\nbest_params = study.best_params\n# best_params already contains 'iterations', 'learning_rate', etc.\nbest_params.update({\n    \"loss_function\": \"Logloss\",\n    \"eval_metric\": \"AUC\",\n    \"verbose\": 0,\n    \"task_type\": \"GPU\",\n    \"devices\": \"0\",\n    \"random_seed\": 42\n})\n\n# Train final model on full training set\nfinal_model = CatBoostClassifier(**best_params)\nfinal_model.fit(\n    X, \n    y, \n    cat_features=cat_features\n)\n\n# Load and preprocess test data\nX_test = df_test[X.columns]\ntest_csv_path = '/kaggle/input/softec-25-machine-learning-competition/test.csv'\ndf_ID = pd.read_csv(test_csv_path)\n\nID = df_ID['ID']\n\n# Predict\ntest_preds = final_model.predict(X_test)\n\n# Building submission DataFrame\nsubmission = pd.DataFrame({\n    \"ID\": ID,\n    \"Readmitted_30\": test_preds\n})\n\n# Save to CSV\nsubmission.to_csv(\"submission_optuna.csv\", index=False)\nprint(\"Wrote submission_optuna.csv!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## üß† Step 4: Final Model Training Using Best Parameters  \n\n- Used the best hyperparameters from Optuna to train on the **full cleaned dataset (X, y)**.  \n- Ensured model used **optimal regularization, structure, and learning rate**.  \n- Enabled `task_type = \"GPU\"` to **speed up training**.  \n\n---\n\n## üì¶ Step 5: Predictions and Submission File  \n\n- Preprocessed the test dataset using the same feature order as training data.  \n- Used the trained model to make predictions.  \n- Created `submission_optuna.csv` containing:  \n  - `ID`: From test set.  \n  - `Readmitted_30`: Predicted label.  ","metadata":{}},{"cell_type":"code","source":"# Split validation set (X, y are full training data)\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, stratify=y, test_size=0.2, random_state=42\n)\n\n# Re-training model on X_train\nfinal_model.fit(X_train, y_train, cat_features=cat_features)\n\n# Predictions\ny_pred = final_model.predict(X_val)\ny_prob = final_model.predict_proba(X_val)[:, 1]\n\n# Computing metrics\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nauc = roc_auc_score(y_val, y_prob)\n\n# Bar Plot of Metrics\nplt.figure(figsize=(8, 5))\nmetrics = {\n    'Precision': precision,\n    'Recall': recall,\n    'F1 Score': f1,\n    'AUC': auc\n}\nsns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='mako')\nplt.ylim(0, 1)\nplt.title(\"Final Model Metrics\")\nfor i, v in enumerate(metrics.values()):\n    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', va='bottom')\nplt.tight_layout()\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_val, y_prob)\nplt.figure(figsize=(7, 5))\nplt.plot(fpr, tpr, label=f'AUC = {auc:.2f}', color='blue')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n# ‚úÖ Summary  \n\n- ‚ùå Removed leaking features that were harming generalization.  \n- üîç Hyperparameters were tuned via Optuna to boost AUC score.  \n- üß† Used CatBoost with built-in categorical support and regularization.  \n- ‚öñÔ∏è Applied class weights to correct class imbalance.  \n- üìà Final model was trained on the full dataset using the best parameters.  \n- üìÅ Submission file generated for competition entry.  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}